{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please use sidpy.viz.plot_utils instead of pyUSID.viz.plot_utils. pyUSID.plot_utils will be removed in a future release of pyUSID\n"
     ]
    }
   ],
   "source": [
    "# Jupyter notebook implementing the full workflow to generate strain maps of inputted micrograph images using the\n",
    "# parent-child algorithm.\n",
    "# Author: Sanket Gadgil, Date: 16/11/2020\n",
    "# With guidance from: https://github.com/pyxem/pyxem-demos/blob/master/05%20Simulate%20Data%20-%20Strain%20Mapping.ipynb\n",
    "%matplotlib qt\n",
    "import hyperspy.api as hs\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import matplotlib.pyplot as plt\n",
    "import pyxem as pxm\n",
    "import diffpy.structure\n",
    "from pyxem.utils.sim_utils import sim_as_signal\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "def pixel_binning(raw_sig, root_of_nbin):\n",
    "    '''\n",
    "    Produce a binned image from an inputted signal micrograph image.\n",
    "    \n",
    "    Arguments:\n",
    "    raw_sig -- input signal image data in the form of a Signal2D instance.\n",
    "    root_of_nbin -- one dimension of the tile that will be averaged, \n",
    "                    eg. if root_of_nbin = 2 and the image dimensions are 256x256 \n",
    "                    then the image will be split into four 128x128 pixel tiles and \n",
    "                    each tile will be averaged to produce a pixel in the binned image, \n",
    "                    which will be of dimensions: 2x2 pixels.\n",
    "\n",
    "    Output:\n",
    "    binned_result -- binned signal, also a Signal2D instance. Note that each of the \n",
    "                     new pixels also has a diffraction pattern which is an average of \n",
    "                     the diffraction patterns of the pixels in the binning tiles.\n",
    "    '''\n",
    "    pixels_binned = int(raw_sig.data.shape[0]/root_of_nbin)\n",
    "    binned_result = raw_sig.rebin(\n",
    "        scale=[pixels_binned, pixels_binned, 1, 1]\n",
    "    )/(pixels_binned**2)  # Normalisation, since pixel binning accumulates without taking the average\n",
    "\n",
    "    return binned_result\n",
    "\n",
    "def log_sig(sig):\n",
    "    '''\n",
    "    Produce a log-scale diffraction patterns for each pixel in an input signal.\n",
    "    \n",
    "    Arguments:\n",
    "    sig -- input signal in the form of a Signal2D instance.\n",
    "    \n",
    "    Output:\n",
    "    logged_sig -- resulting signal with logged diffraction patterns, \n",
    "                  also a Signal2D instance.\n",
    "    '''\n",
    "    \n",
    "    logged_sig = sig  # new variable to prevent overwriting sig\n",
    "    for inav_x in range(logged_sig.data.shape[0]):\n",
    "        for inav_y in range(logged_sig.data.shape[1]):\n",
    "            # The next three lines raise the negative values in the signal to be equal to the \n",
    "            # smallest positive value, this ensures a correct result when taking the log of \n",
    "            # the signal data.\n",
    "            sig_min_indices = logged_sig.data[inav_x, inav_y] <= 0\n",
    "            min_thresh = np.min(logged_sig.data[inav_x, inav_y][logged_sig.data[inav_x, inav_y] > 0])\n",
    "            logged_sig.data[inav_x, inav_y][sig_min_indices] = min_thresh\n",
    "            \n",
    "            logged_sig.data[inav_x, inav_y] = np.log10(logged_sig.data[inav_x, inav_y])\n",
    "    \n",
    "    return logged_sig\n",
    "\n",
    "# Data import (change the path to point to the .xml file in question)\n",
    "sig = hs.load(\"./acquisition_6/acquisition_6.xml\")\n",
    "\n",
    "# Log-scale signal\n",
    "sig = log_sig(sig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binning(sig, scale, firstbin=False):\n",
    "    '''\n",
    "    Produce binned data and additional metadata.\n",
    "    \n",
    "    Arguments:\n",
    "    sig -- input signal, instance of Signal2D.\n",
    "    scale -- binning scale, eg. scale=2 will result in a 2x2 pixel image.\n",
    "    firstbin -- flag signalling whether this is the first binning operation \n",
    "                in the parent-child algorithm.\n",
    "    \n",
    "    Output:\n",
    "    binned_data -- binned signal, instance of Signal2D.\n",
    "    binned_nx, binned_ny -- dimensions of binned_data.\n",
    "    ref_nx, ref_ny -- dimensions of the binned_data at the previous level(parent).\n",
    "                      If firstbin=True these are (1,1).\n",
    "    '''\n",
    "    binned_data = pixel_binning(sig, scale)\n",
    "    binned_nx, binned_ny = binned_data.data.shape[0:2]\n",
    "    \n",
    "    # This returns either the dimensions of the reference data or the dimensions of \n",
    "    # the binned image in the previous level of the parent-child algorithm\n",
    "    if not firstbin:\n",
    "        ref_nx, ref_ny = int(binned_nx/2), int(binned_ny/2)\n",
    "    else:\n",
    "        ref_nx, ref_ny = 1, 1\n",
    "\n",
    "    return [\n",
    "        binned_data,\n",
    "        binned_nx, binned_ny,\n",
    "        ref_nx, ref_ny  # This is needed for model_create()\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyxem.components.scalable_reference_pattern import ScalableReferencePattern\n",
    "\n",
    "def model_create(\n",
    "    binning_results,\n",
    "    reference_data,\n",
    "    first_model=False,\n",
    "    fit_results=None\n",
    "):\n",
    "    '''\n",
    "    Create a hyperspy model2D object from inputted binned signal data and \n",
    "    attach a pyxem ScalableReferencePattern which will be used in latter fitting.\n",
    "    \n",
    "    Arguments:\n",
    "    binning_results -- output from binning().\n",
    "    reference_data -- a reference signal2D object which provides a de facto zero-strain \n",
    "                      diffraction pattern which will be used in latter fitting.\n",
    "    first_model -- flag indicating whether the function call is the first of the \n",
    "                   parent child algorithm.\n",
    "    fit_results -- object of type numpy.ndarray which holds the results of the fitting \n",
    "                   of a previous level(parent) in the parent-child algorithm.\n",
    "    \n",
    "    Output:\n",
    "    model -- model2D object which will be used in model_fit()\n",
    "    '''\n",
    "    \n",
    "    # Unpack arguments\n",
    "    binned_data = binning_results[0]\n",
    "    binned_nx, binned_ny = binning_results[1], binning_results[2]\n",
    "    ref_nx, ref_ny = binning_results[3], binning_results[4] \n",
    "\n",
    "    # Create model and add ScalableReferencePattern\n",
    "    model = binned_data.create_model()\n",
    "    ref_pattern0 = ScalableReferencePattern(reference_data.inav[0, 0])\n",
    "    model.append(ref_pattern0)          \n",
    "    \n",
    "    # Regardless of bounded or unbounded fit, add limits to each parameter\n",
    "    # (they'll only be relevant if bounded=True flag is passed to model_fit())\n",
    "    for component in model:\n",
    "        for param in component.free_parameters:\n",
    "            if param.name == \"d11\":\n",
    "                param.bmin, param.bmax = 0.9, 1.1\n",
    "            elif param.name == \"d12\":\n",
    "                param.bmin, param.bmax = -0.1, 0.1\n",
    "            elif param.name == \"d21\":\n",
    "                param.bmin, param.bmax = -0.1, 0.1\n",
    "            elif param.name == \"d22\":\n",
    "                param.bmin, param.bmax = 0.9, 1.1\n",
    "            elif param.name == \"t1\":\n",
    "                param.bmin, param.bmax = -0.01, 0.01\n",
    "            elif param.name == \"t2\":\n",
    "                param.bmin, param.bmax = -0.01, 0.01\n",
    "    \n",
    "    # inherit parameter data from the previous level of the parent-child algorithm\n",
    "    if not first_model:\n",
    "        d11_data = fit_results[0]\n",
    "        d12_data = fit_results[1]\n",
    "        d21_data = fit_results[2]\n",
    "        d22_data = fit_results[3]\n",
    "        t1_data = fit_results[4]\n",
    "        t2_data = fit_results[5]\n",
    "\n",
    "        # Map the inherited data to new expanded grid eg. (2x2)->(4x4)\n",
    "        for ix in range(ref_nx):\n",
    "            for iy in range(ref_ny):\n",
    "                map_start_ix, map_end_ix = [ix*2, (ix+1)*2]\n",
    "                map_start_iy, map_end_iy = [iy*2, (iy+1)*2]\n",
    "\n",
    "                # The repmat function turns a parameter set at each grid point into a \n",
    "                # repeated (2x2) cluster so as to serve as an initial guess for the next level\n",
    "                # of the parent-child algorithm.\n",
    "                model[0].d11.map[\n",
    "                    map_start_ix:map_end_ix,\n",
    "                    map_start_iy:map_end_iy\n",
    "                ] = repmat(np.array([d11_data[ix, iy]]), 2, 2)\n",
    "                model[0].d12.map[\n",
    "                    map_start_ix:map_end_ix,\n",
    "                    map_start_iy:map_end_iy\n",
    "                ] = repmat(np.array([d12_data[ix, iy]]), 2, 2)\n",
    "                model[0].d21.map[\n",
    "                    map_start_ix:map_end_ix,\n",
    "                    map_start_iy:map_end_iy\n",
    "                ] = repmat(np.array([d21_data[ix, iy]]), 2, 2)\n",
    "                model[0].d22.map[\n",
    "                    map_start_ix:map_end_ix,\n",
    "                    map_start_iy:map_end_iy\n",
    "                ] = repmat(np.array([d22_data[ix, iy]]), 2, 2)\n",
    "                model[0].t1.map[\n",
    "                    map_start_ix:map_end_ix,\n",
    "                    map_start_iy:map_end_iy\n",
    "                ] = repmat(np.array([t1_data[ix, iy]]), 2, 2)\n",
    "                model[0].t2.map[\n",
    "                    map_start_ix:map_end_ix,\n",
    "                    map_start_iy:map_end_iy\n",
    "                ] = repmat(np.array([t2_data[ix, iy]]), 2, 2)\n",
    "\n",
    "    # return a list to allow other outputs to be included if necessary\n",
    "    return [\n",
    "        model,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please ensure parent_child_fit_library.py is either appropriately linked or in the same folder as this notebook.\n",
    "from parent_child_fit_library import multifit\n",
    "\n",
    "def model_fit(\n",
    "    creation_results,\n",
    "    first_fit=False,\n",
    "    bounded=False\n",
    "):\n",
    "    '''\n",
    "    Fit parameters of the model and return the parameters\n",
    "    \n",
    "    Arguments:\n",
    "    creation results -- output from model_create()\n",
    "    first_fit -- flag indicating whether the function call is the first of the \n",
    "                 parent child algorithm. \n",
    "    \n",
    "    Output:\n",
    "    d11_data, d12_data, ... -- Updated parameters.\n",
    "    '''\n",
    "    # Unpack arguments\n",
    "    model = creation_results[0]\n",
    "    \n",
    "    # For independent(linear) fitting the itertype is not relevant but \n",
    "    # is included for consistency, compatibility and comparability.\n",
    "    itertype = \"flyback\"\n",
    "    multifit(model, iterpath=itertype, firstfit=first_fit, bounded=bounded)\n",
    "\n",
    "    # Make copies of the fitted parameter sets to pass to model_create() in the next level\n",
    "    # of the parent-child algorithm\n",
    "    d11_data = deepcopy(model[0].d11.map)\n",
    "    d12_data = deepcopy(model[0].d12.map)\n",
    "    d21_data = deepcopy(model[0].d21.map)\n",
    "    d22_data = deepcopy(model[0].d22.map)\n",
    "    t1_data = deepcopy(model[0].t1.map)\n",
    "    t2_data = deepcopy(model[0].t2.map)\n",
    "\n",
    "    return [\n",
    "        d11_data,\n",
    "        d12_data,\n",
    "        d21_data,\n",
    "        d22_data,\n",
    "        t1_data,\n",
    "        t2_data\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_gen(synthetic=False):\n",
    "    '''\n",
    "    Generate reference image.\n",
    "    \n",
    "    Arguments:\n",
    "    synthetic -- Flag indicating whether to use the synthetic image generated by \n",
    "                 synthetic_reference_image.py or use binning instead.\n",
    "    \n",
    "    Output:\n",
    "    refernce_data -- Reference image as a Signal2D instance.\n",
    "    '''\n",
    "    reference_data = 0\n",
    "\n",
    "    if synthetic:\n",
    "        # Load saved data (change path to the location of the saved data in question)\n",
    "        reference_data = np.loadtxt(\"ref_patt_synth.csv\", delimiter=',')\n",
    "        \n",
    "        # Define metadata to be used when defining the dictionary for the Signal2D instance\n",
    "        ref_dict0 = {'size':1, 'name':'scan_y', 'offset':36, 'scale':72, 'units':'nm'}\n",
    "        ref_dict1 = {'size':1, 'name':'scan_x', 'offset':36, 'scale':72, 'units':'nm'}\n",
    "        ref_dict2 = {'name':'width', 'offset':-0.19, 'scale':0.0029, 'units':'1/nm'}\n",
    "        ref_dict3 = {'name':'height', 'offset':-0.19, 'scale':0.0029, 'units':'1/nm'}\n",
    "\n",
    "        reference_data = hs.signals.Signal2D(np.array([[reference_data]]))\n",
    "\n",
    "        # Enter metadata\n",
    "        for dict_key in ref_dict0.keys():\n",
    "            reference_data.axes_manager[0].__setattr__(dict_key, ref_dict0.get(dict_key))\n",
    "        for dict_key in ref_dict1.keys():\n",
    "            reference_data.axes_manager[1].__setattr__(dict_key, ref_dict1.get(dict_key))\n",
    "        for dict_key in ref_dict2.keys():\n",
    "            reference_data.axes_manager[2].__setattr__(dict_key, ref_dict2.get(dict_key))\n",
    "        for dict_key in ref_dict3.keys():\n",
    "            reference_data.axes_manager[3].__setattr__(dict_key, ref_dict3.get(dict_key))\n",
    "\n",
    "        # Diagnostics (comment out if unnecessary)\n",
    "        reference_data.plot(cmap='jet')\n",
    "    else:\n",
    "        # Normal reference data generated using binning.\n",
    "        reference_data = pixel_binning(sig, 1)\n",
    "        reference_data.plot(cmap=\"jet\")\n",
    "    \n",
    "    return reference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaf7487561b43788bbc33b59a4437e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475267b3dfcb4b0b9d02550c42c40393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4c9e9b71144078a0ded5763c4454ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=64.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fe225548df45bca61566a4a262a15c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922e46e4ee6142349c8a739f3a0a3114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8dd9c17b8f74c9684a9f4864ecbcebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312341d58c404faeb418de1d154b9c62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101c3a1d53bf48088ca73225cda9f19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcc030309084eb5a990d939e64e0e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8babffef9d0f4479bb5aea24cd2663de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d515b92b67c0474a9c0956f91ee7af52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a352ab97dfdf47cb9c5d62812849b084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486f1521fcd54e4ab4c54ddcd6df96e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa15935a925b4a62b01ce62a8dcb280b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n",
      "WARNING:hyperspy.io:`signal_type='tensor_field'` not understood. See `hs.print_known_signal_types()` for a list of known signal types, and the developer guide for details on how to add new signal_types.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StrainMap' object has no attribute 'as_signal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-42f09ef0085f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# convention to allow viewing of log-scale strain maps without missing pixels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mstrain_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_sig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrain_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mstrain_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'jet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StrainMap' object has no attribute 'as_signal'"
     ]
    }
   ],
   "source": [
    "# First level of the parent-child algorithm\n",
    "reference_data = reference_gen()\n",
    "binning_results = binning(sig, 2, firstbin=True)  # (2x2) pixel image to start\n",
    "creation_results = model_create(binning_results, reference_data, first_model=True)\n",
    "fitting_results = model_fit(creation_results, first_fit=True, bounded=True)\n",
    "\n",
    "# For loop allowing iteration of the algorithm up to user chosen limit.\n",
    "# \"2**scale\" defines one of the dimensions of the binned image that is being fitted.\n",
    "# eg. scale=2 ---> (4x4) or scale=4 ---> (16x16)\n",
    "for scale in range(2, 7):\n",
    "    binning_results = binning(sig, int(2**scale))\n",
    "    \n",
    "    # model_create now receives fitting_results from previous level\n",
    "    creation_results = model_create(binning_results, reference_data, first_model=False, fit_results=fitting_results)\n",
    "\n",
    "    fitting_results = model_fit(creation_results, first_fit=False, bounded=True)\n",
    "\n",
    "# unpacking results, though this is from creation_results, \n",
    "# model_fit will have modified the model such that the instance\n",
    "# inside creation_results will be fitted\n",
    "model = creation_results[0]\n",
    "model.as_signal().plot(cmap='jet')  # Diagnostic (comment out if unnecessary)\n",
    "disp_grad = model[0].construct_displacement_gradient()\n",
    "\n",
    "# Strain map generation and plotting\n",
    "strain_map = disp_grad.get_strain_maps()\n",
    "\n",
    "# convention to allow viewing of log-scale strain maps without missing pixels\n",
    "# due to negative values (can comment out if deemed unnecessary)\n",
    "strain_map *= -1\n",
    "\n",
    "strain_map.plot(cmap='jet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
